{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN 특강\n",
    "\n",
    "Fully Connected Layer만 사용시 1차원 형태로 펴지기 때문에 위치정보를 잃어버리게 된다. \n",
    "\n",
    "채널 : R,G,B 채널을 다 더해서 color로 출력가능하다. \n",
    "\n",
    "Feature Map : input data에 filter 합성곱을 통해 feature map을 채운다. \n",
    "\n",
    "Padding : 출력 데이터의 사이즈가 줄어드는 것을 방지하기 위한 것이다.  사이드의 내용이 중요하면 padding을 넣지 않고 그대로 진행한다. \n",
    "\n",
    "Pooling : 원하는 size만큼의 특징을 뽑을 수 있다. 보통 2x2\n",
    "\n",
    "Keras를 이용해서 MNIST data를 분류해보겠다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with Only Fully-Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)\n",
    "\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images,test_labels)  = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image = train_images[0].reshape(28,28)\n",
    "plt.imshow(image,cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(512,activation='relu',input_shape=(28*28,)))\n",
    "network.add(layers.Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000,28*28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000,28*28))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/charming/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 1.4902 - acc: 0.6611\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.5934 - acc: 0.8515\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s 42us/step - loss: 0.4151 - acc: 0.8868\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.3554 - acc: 0.8998\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.3260 - acc: 0.9063\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.3059 - acc: 0.9117\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.2912 - acc: 0.9160\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.2783 - acc: 0.9194\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.2672 - acc: 0.9230\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.2564 - acc: 0.9257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12aa16470>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images,train_labels,epochs=10,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 39us/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 1.7081839944860593 / Test Acc : 0.8915\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Loss : {0} / Test Acc : {1}\".format(test_loss,test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "networkCNN = models.Sequential()\n",
    "\n",
    "# 필터도 학습하면서 최적의 값을 찾아간다. 필터를 weight라고 보면 됨.\n",
    "# strides default 값이 (1,1)이다. \n",
    "\n",
    "networkCNN.add(layers.Conv2D(32,(3,3),activation='relu', input_shape=(28,28,1)))  # (28,28,1)에서 1은 흑백 채널을 말함 / 필터가 32개 있는 것 \n",
    "networkCNN.add(layers.MaxPool2D((2,2)))\n",
    "networkCNN.add(layers.Conv2D(64,(3,3),activation='relu')) \n",
    "networkCNN.add(layers.MaxPool2D((2,2)))\n",
    "networkCNN.add(layers.Conv2D(64,(3,3),activation='relu'))                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_24 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 지금까지 Conv net의 구조 출력 \n",
    "# Conv layer를 거치면 (input_size - Filter_size) / stride) + 1로 사이즈가 줄어듬 \n",
    "# 여기서는 (2,2)로 설정했기에 MaxPooling을 거치면 size가 반으로 줄어든다.  / 소수점은 버림 \n",
    "networkCNN.summary()\n",
    "\n",
    "# param = 3x3x28 = 288 + 32 (필터의 갯수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평평하게 한 줄짜리로 만듬 \n",
    "networkCNN.add(layers.Flatten())\n",
    "networkCNN.add(layers.Dense(64, activation='relu'))\n",
    "networkCNN.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_24 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "networkCNN.summary()\n",
    "# 마지막 softmax를 통해 나온 확률을 통해 레이블을 예측한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "networkCNN.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28,28,1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000,28,28,1))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 31s 515us/step - loss: 1.4089 - acc: 0.5150\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 29s 485us/step - loss: 0.3425 - acc: 0.8947\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 29s 489us/step - loss: 0.1825 - acc: 0.9435\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 29s 490us/step - loss: 0.1213 - acc: 0.9624\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 30s 505us/step - loss: 0.0921 - acc: 0.9717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x139557080>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "networkCNN.fit(train_images,train_labels,epochs=5,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 172us/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = networkCNN.evaluate(test_images,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 0.07805479819169268 / Test Acc : 0.9762\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Loss : {0} / Test Acc : {1}\".format(test_loss,test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN을 이용하니 더 좋은 성능을 내는 것을 확인해 볼 수 있다. \n",
    "\n",
    "89% -> 97%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
